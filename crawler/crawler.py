# -*- coding: UTF-8 -*-.
# http://pt.slideshare.net/roselmamendes/desenvolvendo-web-crawlerscraper-com-python
from bs4 import BeautifulSoup
import requests
import re
import os
import mechanize
from subprocess import call
import urllib

BEGIN = 1
END = 20
goodwares_folder = "../goodwares"

def get_page(url):
    br = mechanize.Browser()
    br.set_handle_equiv(True)
    br.set_handle_gzip(False)
    br.set_handle_redirect(True)
    br.set_handle_referer(True)
    br.set_handle_robots(False)
    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)
    br.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 5.1; rv:14.0) Gecko/20100101 Firefox/14.0.1')]
    br.open(url)
    url_soup = BeautifulSoup(br.response().get_data(), "lxml")
    return url_soup

if not os.path.exists(goodwares_folder):
    os.makedirs(goodwares_folder)
os.chdir(goodwares_folder)

def sourceforge():
    # website base url
    base_url = "http://sourceforge.net"
    # start url - list of softwares
    url = 'http://sourceforge.net/directory/os:windows/?page='
    # loop into softwares list
    for i in range(BEGIN,END):
        try:
            # get softwares list at page i
            url_soup = get_page(url+str(i))
            # get url for every software in page
            for url_a in url_soup.findAll('a', {'itemprop':'url'}):
                # build software page from sourceforge
                item_url = base_url+url_a['href']
                # remove any ?source= from url
                regex = r"(\?source=)\w+.\w+"
                item_url = re.sub(regex, "", item_url)
                # build download page url
                download_url = item_url + "files/latest/download"
                # get download page
                download_soup = get_page(download_url)
                # get download url
                for url_download in download_soup.findAll('a', {'class':'direct-download'}):
                    # get href
                    download_link = url_download['href']
                    if ".exe" in download_link:
                        print download_link
                        # wget download link
                        call(["wget", download_link])
                print
        except e:
            print "Erro em", i
        else:
            pass

def softonic():
    # start url - list of softwares
    url = "https://en.softonic.com/windows/top-downloads/"
    # loop into softwares list
    for i in range(BEGIN,END):
        try:
            # get softwares list at page i
            url_soup = get_page(url+str(i))
            # get li for every software in page
            for url_li in url_soup.findAll('li', {'class':'list-program-item'}):
                # get url for every software in page
                for url_a in url_li.findAll('a'):
                    # build download url
                    download_url = url_a['href']+"/download#downloading"
                    print download_url
                    # get download page html
                    download_soup = get_page(download_url)
                    # find download button and get link
                    for url_download in download_soup.findAll('a',{'id':'download-button'}):
                        # get href
                        download_link = url_download['href']
                        print download_link
                        # check if it's a .exe file
                        if ".exe" in download_link:
                            # remove text after .exe
                            regex = r".exe(.*)"
                            download_name = download_link.split("/")[-1]
                            item_name = re.sub(regex, ".exe", download_name)
                            # try to download file
                            download_file = urllib.URLopener()
                            try:
                                download_file.retrieve(download_link, item_name)
                            except Exception as e:
                                print "Erro ao baixar", item_name
                            else:
                                pass
        except e:
            print "Erro em", i
        else:
            pass
def cnet():
    # start url - list of softwares
    url = "http://download.cnet.com/s/software/windows/?page="
    # loop into softwares list
    for i in range(BEGIN,END):
        try:
            # get softwares list at page i
            url_soup = get_page(url+str(i))
            # get li for every software in page
            for url_a in url_soup.findAll('a', {'class':'item-anchor'}):
                # build download url
                download_url = url_a['href']
                # get download page html
                download_soup = get_page(download_url)
                # find download button and get link
                for url_download in download_soup.findAll('div',{'class':'download-now'}):
                    # get download url
                    download_link = url_download['data-dl-url']
                    # check if it's a .exe file
                    if ".exe" in download_link:
                    # remove text after .exe
                        regex = r".exe(.*)"
                        download_name = download_link.split("/")[-1]
                        item_name = re.sub(regex, ".exe", download_name)
                        # try to download file
                        download_file = urllib.URLopener()
                        try:
                            download_file.retrieve(download_link, item_name)
                        except Exception as e:
                            print "Erro ao baixar", item_name
                        else:
                            pass
        except e:
            print "Erro em", i
        else:
            pass
# download softwares from sourceforge
sourceforge()
# download softwares from softnic
softonic()
# download softwares from cnet
cnet()
